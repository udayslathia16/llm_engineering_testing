{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0c9f6d9-92c9-4f20-ae2a-d38f702a6c4b",
   "metadata": {},
   "source": [
    "Upgrade the website summarizer project to summarize a webpage to use an Open Source model running locally via Ollama rather than OpenAI\n",
    "\n",
    "**Benefits:**\n",
    "1. No API charges - open-source\n",
    "2. Data doesn't leave your box\n",
    "\n",
    "**Disadvantages:**\n",
    "1. Significantly less power than Frontier Model\n",
    "\n",
    "## Recap on installation of Ollama\n",
    "\n",
    "Simply visit [ollama.com](https://ollama.com) and install!\n",
    "\n",
    "Once complete, the ollama server should already be running locally.  \n",
    "If you visit:  \n",
    "[http://localhost:11434/](http://localhost:11434/)\n",
    "\n",
    "You should see the message `Ollama is running`.  \n",
    "\n",
    "If not, bring up a new Terminal (Mac) or Powershell (Windows) and enter `ollama serve`  \n",
    "Then try [http://localhost:11434/](http://localhost:11434/) again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e2a9393-7767-488e-a8bf-27c12dca35bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import requests\n",
    "import ollama\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60880340-7cc8-4d94-a4dc-c5b4b3516a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL=\"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5e793b2-6775-426a-a139-4848291d0463",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Website:\n",
    "    \"\"\"\n",
    "    A utility class to represent a Website that we have scrapped\n",
    "    \"\"\"\n",
    "    url: str\n",
    "    title:str\n",
    "    text: str\n",
    "\n",
    "    def __init__(self,url):\n",
    "        \"\"\"\n",
    "        Create this Website object from the given url using the BeautifulSoup library\n",
    "        \"\"\"\n",
    "        self.url=url\n",
    "        response=requests.get(url)\n",
    "        soup=BeautifulSoup(response.content,'html.parser')\n",
    "        self.title=soup.title.string if soup.title else \"No title found\"\n",
    "        for irrelevant in soup.body([\"script\",\"style\",\"img\",\"input\"]):\n",
    "            irrelevant.decompose()\n",
    "        self.text = soup.body.get_text(separator=\"\\n\",strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ef960cf-6dc2-4cda-afb3-b38be12f4c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is Ollama? Introduction to the AI model management tool\n",
      "WordPress\n",
      "VPS\n",
      "Website Development\n",
      "eCommerce\n",
      "Website Errors\n",
      "How to Make a Website\n",
      "search\n",
      "VPS\n",
      "VPS for Web Devs\n",
      "Nov 05, 2024\n",
      "Ariffud M.\n",
      "6min                      Read\n",
      "What is Ollama? Understanding how it works, main features and models\n",
      "Copy link\n",
      "Copied!\n",
      "Ollama is an open-source tool that runs large language models (LLMs) directly on a local machine. This makes it particularly appealing to AI developers, researchers, and businesses concerned with data control and privacy.\n",
      "By running models locally, you maintain full data ownership and avoid the potential security risks associated with cloud storage. Offline AI tools like Ollama also help reduce latency and reliance on external servers, making them faster and more reliable.\n",
      "This article will explore Ollama’s key features, supported models, and practical use cases. By the end, you’ll be able to determine if this LLM tool suits your AI-based projects and needs.\n",
      "Download ChatGPT cheat sheet\n",
      "How Ollama works\n",
      "Key features of Ollama\n",
      "Local AI model management\n",
      "Command-line and GUI options\n",
      "Multi-platform support\n",
      "Available models on Ollama\n",
      "Use cases for Ollama\n",
      "Benefits of using Ollama\n",
      "What is Ollama FAQ\n",
      "What is Ollama AI used for?\n",
      "Can I customize the AI models in Ollama?\n",
      "Is Ollama better than ChatGPT?\n",
      "How Ollama works\n",
      "Ollama creates an isolated environment to run LLMs locally on your system, which prevents any potential conflicts with other installed software. This environment already includes all the necessary components for deploying AI models, such as:\n",
      "Model weights\n",
      ". The pre-trained data that the model uses to function.\n",
      "Configuration files\n",
      ". Settings that define how the model behaves.\n",
      "Necessary dependencies\n",
      ". Libraries and tools that support the model’s execution.\n",
      "To put it simply, first – you pull models from the Ollama library. Then, you run these models as-is or adjust parameters to customize them for specific tasks. After the setup, you can interact with the models by entering prompts, and they’ll generate the responses.\n",
      "This advanced AI tool works best on\n",
      "discrete graphical processing unit (GPU) systems\n",
      ". While you can run it on CPU-integrated GPUs, using dedicated compatible GPUs instead, like those from NVIDIA or AMD, will reduce processing times and ensure smoother AI interactions.\n",
      "We recommend checking\n",
      "Ollama’s official GitHub page\n",
      "for GPU compatibility.\n",
      "Key features of Ollama\n",
      "Ollama offers several key features that make offline model management easier and enhance performance.\n",
      "Local AI model management\n",
      "Ollama grants you full control to download, update, and delete models easily on your system. This feature is valuable for developers and researchers who prioritize strict data security.\n",
      "In addition to basic management, Ollama lets you track and control different model versions. This is essential in research and production environments, where you might need to revert to or test multiple model versions to see which generates the desired results.\n",
      "Command-line and GUI options\n",
      "Ollama mainly operates through a\n",
      "command-line interface (CLI)\n",
      ", giving you precise control over the models. The CLI allows for quick commands to pull, run, and manage models, which is ideal if you’re comfortable working in a terminal window.\n",
      "Ollama also supports third-party graphical user interface (GUI) tools, such as\n",
      "Open WebUI\n",
      ", for those who prefer a more visual approach.\n",
      "Multi-platform support\n",
      "Another standout feature of Ollama is its broad support for various platforms, including macOS, Linux, and Windows.\n",
      "This cross-platform compatibility ensures you can easily integrate Ollama into your existing workflows, regardless of your preferred operating system. However, note that Windows support is currently in preview.\n",
      "Additionally, Ollama’s compatibility with Linux lets you\n",
      "install it on a virtual private server (VPS)\n",
      ". Compared to running Ollama on local machines, using a VPS lets you access and manage models remotely, which is ideal for larger-scale projects or team collaboration.\n",
      "Available models on Ollama\n",
      "Ollama supports numerous ready-to-use and customizable\n",
      "large language models\n",
      "to meet your project’s specific requirements. Here are some of the most popular Ollama models:\n",
      "Llama 3.2\n",
      "Llama 3.2 is a versatile model for natural language processing (NLP) tasks, like text generation, summarization, and machine translation. Its ability to understand and generate human-like text makes it popular for developing chatbots, writing content, and building conversational AI systems.\n",
      "You can fine-tune Llama 3.2 for specific industries and niche applications, such as customer service or product recommendations. With solid multilingual support, this model is also favored for building machine translation systems that are useful for global companies and multinational environments.\n",
      "Mistral\n",
      "Mistral handles code generation and large-scale data analysis, making it ideal for developers working on AI-driven coding platforms. Its pattern recognition capabilities enable it to tackle complex programming tasks, automate repetitive coding processes, and identify bugs.\n",
      "Software developers and researchers can customize Mistral to generate code for different programming languages. Additionally, its data processing ability makes it useful for managing large datasets in the finance, healthcare, and eCommerce sectors.\n",
      "Code Llama\n",
      "As the name suggests, Code Llama excels at programming-related tasks, such as writing and reviewing code. It automates coding workflows to boost productivity for software developers and engineers.\n",
      "Code Llama integrates well with existing development environments, and you can tweak it to understand different coding styles or programming languages. As a result, it can handle more complex projects, such as API development and system optimization.\n",
      "LLaVA\n",
      "LLaVA is a\n",
      "multimodal model\n",
      "capable of processing text and images, which is perfect for tasks that require visual data interpretation. It’s primarily used to generate accurate image captions, answer visual questions, and enhance user experiences through combined text and image analysis.\n",
      "Industries like eCommerce and digital marketing benefit from LLaVA to analyze product images and generate relevant content. Researchers can also adjust the model to interpret medical images, such as X-rays and MRIs.\n",
      "Phi-3\n",
      "Phi-3 is designed for scientific and research-based applications. Its training on extensive academic and research datasets makes it particularly useful for tasks like literature reviews, data summarization, and scientific analysis.\n",
      "Medicine, biology, and environmental science researchers can fine-tune Phi-3 to quickly analyze and interpret large volumes of scientific literature, extract key insights, or summarize complex data.\n",
      "If you’re unsure which model to use, you can explore\n",
      "Ollama’s model library\n",
      ", which provides detailed information about each model, including installation instructions, supported use cases, and customization options.\n",
      "Suggested reading\n",
      "For the best results when building advanced AI applications, consider combining LLMs with\n",
      "generative AI\n",
      "techniques. Learn more about it in our article.\n",
      "Use cases for Ollama\n",
      "Here are some examples of how Ollama can impact workflows and create innovative solutions.\n",
      "Creating local chatbots\n",
      "With Ollama, developers can create highly responsive AI-driven chatbots that run entirely on local servers, ensuring that customer interactions remain private.\n",
      "Running chatbots locally lets businesses avoid the latency associated with cloud-based AI solutions, improving response times for end users. Industries like transportation and education can also fine-tune models to fit specific language or industry jargon.\n",
      "Conducting local research\n",
      "Universities and data scientists can leverage Ollama to conduct offline machine-learning research. This lets them experiment with datasets in privacy-sensitive environments, ensuring the work remains secure and is not exposed to external parties.\n",
      "Ollama’s ability to run LLMs locally is also helpful in areas with limited or no internet access. Additionally, research teams can adapt models to analyze and summarize scientific literature or draw out important findings.\n",
      "Building privacy-focused AI applications\n",
      "Ollama provides an ideal solution for developing privacy-focused AI applications that are ideal for businesses handling sensitive information. For instance, legal firms can create software for contract analysis or legal research without compromising client information.\n",
      "Running AI locally guarantees that all computations occur within the company’s infrastructure, helping businesses meet regulatory requirements for data protection, such as GDPR compliance, which mandates strict control over data handling.\n",
      "Integrating AI into existing platforms\n",
      "Ollama can easily integrate with existing software platforms, enabling businesses to include AI capabilities without overhauling their current systems.\n",
      "For instance, companies using content management systems (CMSs) can integrate local models to improve content recommendations, automate editing processes, or suggest personalized content to engage users.\n",
      "Another example is integrating Ollama into customer relationship management (CRM) systems to enhance automation and data analysis, ultimately improving decision-making and customer insights.\n",
      "Suggested reading\n",
      "Did you know that you can\n",
      "create your own AI application, like ChatGPT\n",
      ", using OpenAI API? Learn how to do so in our article.\n",
      "Benefits of using Ollama\n",
      "Ollama provides several advantages over cloud-based AI solutions, particularly for users prioritizing privacy and cost efficiency:\n",
      "Enhanced privacy and data security\n",
      ". Ollama keeps sensitive data on local machines, reducing the risk of exposure through third-party cloud providers. This is crucial for industries like legal firms, healthcare organizations, and financial institutions, where data privacy is a top priority.\n",
      "No reliance on cloud services\n",
      ". Businesses maintain complete control over their infrastructure without relying on external cloud providers. This independence allows for greater scalability on local servers and ensures that all data remains within the organization’s control.\n",
      "Customization flexibility\n",
      ". Ollama lets developers and researchers tweak models according to specific project requirements. This flexibility ensures better performance on tailored datasets, making it ideal for research or niche applications where a one-size-fits-all cloud solution may not be suitable.\n",
      "Offline access\n",
      ". Running AI models locally means you can work without internet access. This is especially useful in environments with limited connectivity or for projects requiring strict control over data flow.\n",
      "Cost savings\n",
      ". By eliminating the need for cloud infrastructure, you avoid recurring costs related to cloud storage, data transfer, and usage fees. While cloud infrastructure may be convenient, running models offline can lead to significant long-term savings, particularly for projects with consistent, heavy usage.\n",
      "Conclusion\n",
      "Ollama is ideal for developers and businesses looking for a flexible, privacy-focused AI solution. It lets you run LLMs locally and provides complete control over data privacy and security.\n",
      "Additionally, Ollama’s ability to adjust models makes it a powerful option for specialized projects. Whether you’re developing chatbots, conducting research, or building privacy-centric applications, it offers a cost-effective alternative to cloud-based AI solutions.\n",
      "Finally, if you’re looking for a tool that offers both control and customization for your AI-based projects, Ollama is definitely worth exploring.\n",
      "What is Ollama FAQ\n",
      "What is Ollama AI used for?\n",
      "Ollama runs and manages large language models (LLMs) locally on your machine. It’s ideal for users who want to avoid cloud dependencies, ensuring complete control over data privacy and security while maintaining flexibility in AI model deployment.\n",
      "Can I customize the AI models in Ollama?\n",
      "Yes, you can customize AI models in Ollama using the\n",
      "Modelfile system\n",
      ". This system lets you modify models to fit specific project needs, adjust parameters, or even create new versions based on existing ones.\n",
      "Is Ollama better than ChatGPT?\n",
      "Ollama offers a privacy-focused alternative to ChatGPT by running models and storing data on your system. While ChatGPT provides more scalability through cloud-based infrastructure, it may raise concerns about data security. The better choice depends on your project’s privacy and scalability needs.\n",
      "The author\n",
      "Ariffud Muhammad\n",
      "Ariffud is a Technical Content Writer with an educational background in Informatics. He has extensive expertise in Linux and VPS, authoring over 200 articles on server management and web development. Follow him on\n",
      "LinkedIn\n",
      ".\n",
      "More from Ariffud Muhammad\n",
      "Copy link\n",
      "Copied!\n",
      "Related tutorials\n",
      "18 Nov •\n",
      "VPS\n",
      "•\n",
      "VPS for Web Devs\n",
      "•\n",
      "How to install CyberPanel? Using a VPS template and manually\n",
      "CyberPanel provides a graphical interface and various features to streamline Linux virtual private server (VPS) management. However, setting up this...\n",
      "By Aris Sentika\n",
      "18 Nov •\n",
      "VPS\n",
      "•\n",
      "Gaming\n",
      "•\n",
      "How to OP someone in Minecraft: using Game Panel and OP commands\n",
      "In Minecraft, giving a player operator status, or “OPing” them, is a simple yet powerful way to grant administrative privileges within a...\n",
      "By Dominykas Jasiulionis\n",
      "15 Nov •\n",
      "VPS\n",
      "•\n",
      "VPS for Web Devs\n",
      "•\n",
      "Ollama GUI tutorial: How to set up and use Ollama with Open WebUI\n",
      "By default, Ollama runs large language models (LLMs) through a command-line interface (CLI). However, you can pair Ollama with Open WebUI – a...\n",
      "By Ariffud Muhammad\n",
      "What our customers say\n",
      "Trustpilot\n",
      "Leave a reply\n",
      "Cancel reply\n",
      "Please fill the required fields.\n",
      "Please accept the privacy checkbox.\n",
      "Please fill the required fields and accept the privacy checkbox.\n",
      "Δ\n",
      "Thank you! Your comment has been successfully submitted. It will be approved within the next 24 hours.\n"
     ]
    }
   ],
   "source": [
    "# ed=Website(\"https://www.scirp.org/journal/paperinformation?paperid=31320\")\n",
    "ed=Website(\"https://www.hostinger.in/tutorials/what-is-ollama\")\n",
    "print(ed.title)\n",
    "print(ed.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efff96db-5997-4713-8f84-24b97b4175e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a478a0c-2c53-48ff-869c-4d08199931e1",
   "metadata": {},
   "source": [
    "## Types of prompts\n",
    "\n",
    "You may know this already - but if not, you will get very familiar with it!\n",
    "\n",
    "Models like GPT4o have been trained to receive instructions in a particular way.\n",
    "\n",
    "They expect to receive:\n",
    "\n",
    "**A system prompt** that tells them what task they are performing and what tone they should use\n",
    "\n",
    "**A user prompt** -- the conversation starter that they should reply to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "abdb8417-c5dc-44bc-9bee-2e059d162699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our system prompt - you can experiment with this later, changing the last sentence to 'Respond in markdown in Spanish.\"\n",
    "\n",
    "system_prompt=\"You are an assistant that analyzes the contents of a website \\\n",
    "    and provide a short summary, ignoring text that might be navigation related. \\\n",
    "        Respond in markdown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0275b1b-7cfe-4f9d-abfa-7650d378da0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that writes a User Prompt that asks for summaries of websites:\n",
    "# See how this function creates exactly the format above\n",
    "\n",
    "def user_prompt_for(website):\n",
    "    user_prompt=f\"You are looking at a website titled {website.title}\"\n",
    "    user_prompt +=\"The contents of this website is as follows; \\\n",
    "please provide a short summary of this website in markdown. \\\n",
    "If it includes news or announcements, then summarize these too.\\n\\n\"\n",
    "    user_prompt +=website.text\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea211b5f-28e1-4a86-8e52-c0b7677cadcc",
   "metadata": {},
   "source": [
    "## Messages\n",
    "\n",
    "The API from Ollama expects to receive messages in format as OpenAI\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message goes here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user message goes here\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0134dfa4-8299-48b5-b444-f2a8c3403c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how this function creates exactly the format above\n",
    "def messages_for(website):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(website)}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f49d46-bf55-4c3e-928f-68fc0bf715b0",
   "metadata": {},
   "source": [
    "## Time to bring it together - now with Ollama instead of OpenAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "905b9919-aba7-45b5-ae65-81b3d1d78e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now: call the OpenAI API. You will get very familiar with this!\n",
    "def summarize(url):\n",
    "    website=Website(url)\n",
    "    \n",
    "    response=ollama.chat(\n",
    "        model=model,\n",
    "        messages=messages_for(website)\n",
    "    )\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05e38d41-dfa4-4b20-9c96-c46ea75d9fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ollama is an open-source, locally hosted AI solution that allows users to run large language models (LLMs) on their own machines, providing a more private and cost-effective alternative to cloud-based AI services.\\n\\n**Benefits of using Ollama:**\\n\\n1. **Enhanced privacy and data security**: Ollama keeps sensitive data on local machines, reducing the risk of exposure through third-party cloud providers.\\n2. **No reliance on cloud services**: Businesses maintain complete control over their infrastructure without relying on external cloud providers.\\n3. **Customization flexibility**: Ollama lets developers and researchers tweak models according to specific project requirements.\\n4. **Offline access**: Running AI models locally means you can work without internet access.\\n5. **Cost savings**: By eliminating the need for cloud infrastructure, users avoid recurring costs related to cloud storage, data transfer, and usage fees.\\n\\n**Who is Ollama suitable for?**\\n\\n1. Developers and researchers who require a high degree of customization and control over their AI models.\\n2. Businesses that prioritize data security and privacy, particularly in industries such as healthcare, finance, and government.\\n3. Users who need to run large language models without relying on cloud-based infrastructure.\\n\\n**Comparison with ChatGPT:**\\n\\nWhile both Ollama and ChatGPT provide AI-powered solutions, Ollama offers a more private and cost-effective alternative by running models locally on the user's machine. The better choice depends on the project's specific needs for scalability, privacy, and customization.\\n\\n**Other resources:**\\n\\n* Ollama GitHub repository\\n* Ollama documentation\\n\\nOverall, Ollama is an attractive option for users who value data security, customization, and cost-effectiveness in their AI solutions.\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(\"https://www.hostinger.in/tutorials/what-is-ollama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d926d59-450e-4609-92ba-2d6f244f1342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to display this nicely in the Jupyter output, using markdown\n",
    "def display_summary(url):\n",
    "    summary=summarize(url)\n",
    "    display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3018853a-445f-41ff-9560-d925d1774b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_summary(\"https://en.wikipedia.org/wiki/Large_language_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bcf6f4-adce-45e9-97ad-d9a5d7a3a624",
   "metadata": {},
   "source": [
    "# Let's try more websites\n",
    "\n",
    "Note that this will only work on websites that can be scraped using this simplistic approach.\n",
    "\n",
    "Websites that are rendered with Javascript, like React apps, won't show up. See the community-contributions folder for a Selenium implementation that gets around this. You'll need to read up on installing Selenium (ask ChatGPT!)\n",
    "\n",
    "Also Websites protected with CloudFront (and similar) may give 403 errors - many thanks Andy J for pointing this out.\n",
    "\n",
    "But many websites will work just fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d83403-a24c-44b5-84ac-961449b4008f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_summary(\"https://www.aajtak.in/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e9fd40-b354-4341-991e-863ef2e59db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_summary(\"https://anthropic.com\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
